# 4. Etude des cas grenoblois et lyonnais permettant de comprendre les attentes des opérateurs de l'open data vis à vis d'une intervention en Systèmes d'Informations permettant de faciliter l'usage de la donnée

- [X] Distinguer l'introduction méthodologique (connaissances issue de la phase explo --> connaissance de la phase étude de cas) VS. rappel du fonc, càd conclusions de la phase exploratoire qui précède la première itération (et non les 3)

Dans le chapitre précédent, nous avons mené une étude de cas exploratoire à l'échelle du territoire français. 
Elle nous a permis de représenter le contexte (C) dans lequel se situe la situation problématique (Carlsson, 2010) adréssée par cette thèse. L'usage de la donnée est réalisé par des **opérateurs de l'open data** s'inscrit dans le cadre d'**écosystèmes open data** . A travers des entretiens semi-directifs, des observations participantes et des analyses de documents, nous avons établi une **heuristique** de ces écosystèmes. Cette représentation décrit les différents sous-systèmes qui compose un écosystème, les différentes phases de son évolution et les mécanismes qui les régissent. Aussi, cette étude de cas exploratoire nous a permis d'établir les deux unités d'analyse pertinentes pour avoir une compréhension à la flois globale et détaillée de la situation problématique : 

- les interactions homme-machine (human computer interaction) entre les opérateurs de l'open data et les technologies qu'ils utilisent et ce lors des différentes phases de l'**usage des données/open data work**
- les interactions entre les agents des différents sous-systèmes nécessaires lors de l'usage des données/open data work

Métaphoriquement, ces deux éléments constituent la carte de la situation problématique. L'heuristique est à la situation problématique ce que la topographie est à une carte : une représentation normée de ces différentes aires et reliefs. Aussi, les deux unités d'analyse sont à la situation problématique ce qu'une échelle est à une carte : un moyen de mesure permettant de décrire des phénomènes en deux dimensions. 


Or, la carte n'est pas le territoire (Korzybski, 1998) et ce chapitre vise à constuire le deuxième sur la base de la première. Pour cela, il est question de réaliser une étude de cas comparative des écosystèmes open data de la métropôle Grenoble-Alpes et du Grand Lyon. Comme l'indique l'architecture générale de cette thèse, elle a pour but premier, dans chaque itération, de construire les besoins fonctionnels auquel une intervention en système d'information doit répondre pour passer de la situation problématique à la situation désirée. Ces besoins fonctionnels comportent trois éléments : une description du contexte (C) social et technique, des différents éléments de situation problématique (ESP) ainsi que des solutions (S) entrevues par les opérateurs de l'open data pour atteindre la situation désirée (SD). *In fine*, cette étude de cas nous permettra d'établir une liste de besoins fonctionnels qui seront exprimés de la façon suivante : 

> Pour atteindre la SD dans C, il s'agit d'adresser ESP en mettant en place S
> - **Exemple de besoin fonctionnel, inspirée de la logique CIMO créée par Denyer et al. (2008) pour exprimer des propositions de design**

Pour aboutir à ces besoins fonctionnels, nous mettrons en place, pour chaque itération, une méthodologie d’étude de cas qui s’inspire principalement des travaux d’Eisenhardt (1989). La particularité de ses travaux consiste à combiner les travaux de Miles et Huberman sur l’analyse des données qualitatives (1984), ceux de Yin (1981, 1984) sur le design d’études de cas et les recherches de Glaser et Strauss (1967) sur le développement de grounded theories. Elle mobilise tour à tour ces travaux pour proposer une feuille de route qui précise les étapes à suivre pour construire des connaissances théoriques à partir d’une étude de cas. Dans notre étude de cas, les connaissances théoriques prennent la forme des besoins fonctionnels mentionnés ci-dessus.

Nous reprenons les 9 étapes proposées par Eisenhardt (1989) : 

1. Définition de la sous-question de recherche adressée
2. Sélection des cas 
3. Construction du protocole et des instruments de l’étude de cas
4. Immersion dans le terrain 
5. Analyses intra-cas
6. Recherche de pattern inter-cas
7. Ebauche d’hypothèses
8. Confrontation avec la littérature 
9. Atteinte du point de clôture 


### 1ère étape : définition de la sous-question de recherche adressée
- [X] done - adapter en fonction de l'itération

La première étape consiste à rappeler la sous-question de recherche adressée par cette étude de cas. Elle se définit eu égard à notre compréhension de la situation problématique, de son contexte, et change avec elle. En effet, pour rappel, notre méthodologie de recherche en *design science* est intrinsèquement itérative. A la fin de chaque itération (chapitre 7), notre compréhension de la situation problématique se déplace et se précise. Elle évolue pour renforcer graduellement sa **pertinence** vis à vis de la réalité des practiciens auxquels cette thèse s'adresse. De ce fait, la sous-question de recherche sera redéfinie en fonction des évolutions de notre compréhension de la situation problématique. 

Grâce à notre étude de cas exploratoire, nous avons posé comme première hypothèse (à ne pas prendre dans un sens positiviste) que le faible usage de l'open data était du à la présence de barrières socio-techniques dans les écosystèmes open data. Par conséquent, le première itération de l'étude de cas vise à répondre à la sous-question de recherche suivante : 

> Quels sont les besoins fonctionnels auxquels une intervention en système d'information doit répondre pour lever les barrières socio-techniques qui se dressent face à l'usage des données ouvertes ? 

A nouveau, répetons que cette première question sera précisée ou complètement redéfinie dans les prochaines itérations en fonction de la reconceptualisation de la situation problématique, elle-même issue des phases d'évaluation (chapitre 7  de l'intervention que nous designerons (chapitre 5) et prototyperons (chapitre 6)

### 2ème étape : sélection des cas
- [X] done - expliquer pourquoi Grenoble et Lyon

Contrairement à la première étape dont le contenu évoluera au gré des itérations, cette deuxième étape est statique. En effet, nous n'étudierons que les cas des écosystèmes open data de la métropôle Grenoble-Alpes et du Grand Lyon. Cette sélection de cas se justifie, au-delà de leur facilité d'accès pour l'équipe de recherche, par le fait que ces deux ecosystèmes possèdent un double avantage. D'un côté, ils sont suffisamment mures pour qu'on puisse y observer des phénomènes d'usage des données. De l'autre, ils sont suffisamment hétérogènes en termes de géométrie, de mode de gestion et d'orientation, pour que les résultats communs aux deux cas puissent prétendre à une certaine validité externe. 

- PhD Working Book (4) - p.82-93 - Méthodo de constitution de l'échantillon

### 3ème étape : construction du protocole et des instruments de l’étude de cas
- [X] done - expliquer la variation du protocole en fonction de l'itération 

Nous solliciterons plusieurs modes de collecte de données hétérogènes de façon à béneficier d’une triangulation des données. Aussi cette pluralité d'instruments nous permettera de renforcer la validité de nos construits et nous permettera d’établir les différentes dimensions de l'usage de la donnée que nous étudions ici. 

- PhD Working Book (4) - p.82-93 - Méthodo de constitution de l'échantillon

A nouveau, cette étape verra son contenu varier en fonction des évolutions de notre compréhension de la situation problématique et donc de la sous-question de recherche adressée par l'étude de cas. Cela étant, au commencement, dans le cadre de notre première itération, nous avons : 

- réalisé des entretiens semi-directifs en suivant un protocole de collecte et d'analyse de contenu thématique (Miles et Huberman, 2004) cadré par l'heuristique d'écosystème open data et centré autour des deux unités d'analyses choisies dans le chapitre précédent. 
- conduit des analyses de document internes produits directement par des opérateurs de l'open data à Lyon et à Grenoble. Nous avons également suivi un protocole d'analyse de contenu thématique (Miles et Huberman, 2004)


### 4ème étape : immersion dans le terrain
- [X] done - expliquer la variation du protocole en fonction de l'itération 

A l'image de cette thèse, la phase de collecte de donnée et d'immersion dans le terrain a évolué entre chaque itération. Dans. Nous avons orienté nos recherches en fonctions des opportunité de collecte suggérées par l'itération précedente. A ce propos, Einsenhardt rappelait que pour les recherches dans lesquelles il est question de construire un cadre théorique, cette flexibilité dans la collecte des données était parfaitement légitime : 

> For theory-building research, (altering data collection methods) is legitimate because investigators are trying to understand each case individually and as much depth as is feasible. Thus, if a new data collection opportunity arises and if a new line of thinking emerges during the research, it makes sense to take advantage by altering data collection, if such an alteration is likely to better ground the theory or to provide new theoretical insight”
> - Eisenhardt (1984, p. 539)


### 5ème étape : analyse intra-cas
- [X] done - expliquer la variation du protocole en fonction de l'itération 

Dans cette cinquième phase, si les objectifs de l'analyse évolueront au gré des itérations, le protocole sera le même. 
Nous commencerons par établir une longue description des cas qui aidera à traiter les larges volumes de données récoltées. Ensuite, nous reprendrons la méthode utilisée par Leonard-Barton (1988) qui utilise des tableaux et des graphiques d’information pour représenter les données. 

### 6ème cas : recherche de pattern inter-cas
- [X] done - expliquer la variation du protocole en fonction de l'itération 

Dans la même veine que dans la cinquième étape, si les objectifs de l'analyse évolueront au gré des itérations, le protocole sera le même. Nous utiliserons ici la tactique qui consiste à sélectionner des catégories ou des dimensions particulières puis de confronter les similitudes intra-cas avec les différences entre les cas(Eisenhardt, 1984, p.540). Cette méthode nous permettra d’identifier aussi bien des traits spécifiques que des traits communs à nos deux cas. 


### 7ème cas : Ebauche d'hypothèses
- [X] done - expliquer la variation du protocole en fonction de l'itération 

A nouveau, si le contenu des hypothèses évoluera au gré des itérations, le protocole d'ébauche sera le même. Cette étape hautement itérative consiste à comparer systématiquement les résultats émergents, avec les preuves issues de chaque cas, de façon à évaluer dans quelle mesure les résultats épousent les données récoltées. Il s’agit ici de comparer continuellement la littérature sur les besoins fonctionnels avec les données de façon à renforcer la validité empirique des premiers. 
Pour cela, je raisonnerai en 2 étapes. La première étape, elle-même subdivisée en deux sous-parties, consiste à affiner les besoins fonctionnels. La première sous-partie consiste à affiner la définition des besoins fonctionnels avec la littérature, la deuxième sous-partie à apporter la preuve que les données évoquent des problèmes qui peuvent être levées par ces besoins fonctionnels. La deuxième étape consiste pour sa part à s’assurer que les relations entre les besoins fonctionnels épousent bien les relations entre les problèmes évoqués. 


### 8ème cas : Confrontation avec la littérature
- [X] done - expliquer la variation du protocole en fonction de l'itération 

Dans la continuité, si le contenu de la littérature à laquelle les hypothèses seront confrontées évoluera au gré des itérations, le protocole de confrontation sera le même.
Cette étape consiste à confronter les besoins fonctionnels émergents avec la littérature existante sur les artefacts facilitant l’usage de la donnée. Il s’agit plus particulièrement d’extraire d’articles de recherche les besoins fonctionnels auxquels les artefacts se sont adaptés. 

Cette phase de confrontation permettra d’identifier les similitudes ou au contraire les contradictions avec cette littérature. En juxtaposant des résultats contredisant la littérature ou inversement, nous serons amené à penser différemment, de manière plus créative et ce, de façon à préciser l’expression des besoins fonctionnels identifiés. 


### 9ème cas : Atteinte du point de clôture 
- [X] done - expliquer la variation du protocole en fonction de l'itération 

Enfin, cette dernière étape consiste à décider quand il s’agit d’arrêter les itérations entre la littérature et les données. Idéalement, cette étape s’arrêtera quand on atteindra la saturation théorique, c’est à dire quand l’apprentissage marginal de chaque nouvelle donnée sera minime voire nulle. 
