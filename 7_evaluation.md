# 7. Evaluations visant à estimer la capacité d'Innodata à faciliter les interactions entre les opérateurs de l'open data et les *data spaces*

Dans le chapitre précédent, nous avons détaillé le **processus de prototypage** qui nous a amené à concevoir l'**artefact**. Aussi avons-nous décrit formellement la structure de cet artefact et expliquer en quoi ces différents éléments pourraient répondre aux différents aspects de la question. Ces explications sont autant de réponses temporaires dont il s'agit d'évaluer la **pertinence empirique** et la **rigueur scientifique** dans ce chapitre. 

Nous avons mené des évaluations dans le but d'estimer successivement la capacité des différentes versions d'Innodata (i.e les artefacts) à conduire l'innovation de services issue de l'open data dans les villes intelligentes vers l'idéal qu'elles se sont fixées. En effet, le choix de la *design science research methodology* suppose d'adopter une démarche intrinséquement itérative dans laquelle une première réponse artefactuelle est apportée avant d'être évaluée. Si la réponse est suffisamment pertinente empiriquement et rigoureuse scientifiquement, alors la recherche s'arrête. Dans le cas contraire, la question de recherche est précisée et un nouveau cycle de recherche démarre. Cette nouvelle itération apporte une deuxième réponse artefactuelle qui est à son tour évaluée, et ainsi de suite.

- [x] Décrire la structure générale des évaluations menées à la fin de chaque cycle de recherche. 
- [x] Expliquer le lien avec la partie prototypage et avec la première étape du nouveau cycle 
- [x] Dédoubler en évaluation de la pertinence empirique et de la rigueur scientifique 

La stratégie globale d'évaluation des artefacts que nous avons mis en place (et qui s'est décliné en fonction des spécificités de chaque itération) reprend les travaux de Venable et al. (2012). Ils proposent un processus de décision éprouvé qui permet d'établir une stratégie d'évaluation d'artefact. Nous l'avons appliqué à chaque itération. Nous aurions pu également nous baser sur les travaux de Pries-Heje (2008). Or, ces derniers proposent un processus de décision moins exhaustif et des stratégies d'évaluation pour un nombre restreint de types de projets de recherche en *design science*. Le processus de Venable et al. (2012) est dès lors plus adaptable aux spécificités de cette thèse. 

### Première étape de construction de la stratégie d'évaluation : analyse de l'évaluation
La première étape du processus de décision consiste, une fois les activités de prototypage terminées, à évaluer le contexte et les objectifs de la phase d'évaluation. Venable et al. (2012) préconise de déterminer les éléments suivants et leur importance relative : 

1. 
2. 
3. 
4. 
5. 
6. 
7. 

### Deuxième étape de construction de la stratégie d'évaluation : choix d'une stratégie
Une fois ces éléments identifiés, il s'agit de se reporter au *DSR Evaluation Strategy Selection Framework* qui fait le lien entre ces critères et différentes stratégies possibles pour l'évaluation d'un artefact : évaluation *ex ante* ou *ex post*, évaluation naturaliste ou artificielle. Chaque itération se voit alors attribuée un quadrant avec les caractéristiques de la(les) stratégie(s) choisie(s)

### Troisième étape de construction de la stratégie d'évaluation : choix des méthodes
A ce premier framework correspond un second, le *DSR Evaluation Method Selection Framework* qui fait correspondre à la stratégie choisie une liste de méthodes d'évaluations appropriéés. Il s'agit ensuite de choisir la (ou les) méthode(s) à appliquer à l'artefact. 

### Quatrième étape de construction de la stratégie d'évaluation : design du détail de l'évaluation 
Dans cette étape, il s'agit d'articuler les différentes stratégies et méthodes choisies de façon à former un design cohérent. Il s'agit également d'apprendre le fonctionnement des méthodes chosies et de les designer individuellement. 

### Cinquième étape de construction de la stratégie d'évaluation : réalisation des évaluations
Les méthodes choisies sont mises en place selon le design choisies. On collecte les données avant de les analyser et de tirer des conclusions qui visent à savoir dans quelle mesure l'artefact a répondu à la sous-question de recheche qui structure l'itération. 

### Sixièmé étape de construction de la stratégie d'évaluation : implication sur la prochaine itération
Dans cette dernière étape, on évalue la pertinence empirique et la rigueur scientifique de l'artefact avant de noter spécifiquement les manquements relatifs à ces deux aspects qui sont à améliorer dans la prochaine itération. 


- [ ] Terminer sur le "quand s'arrêter"
